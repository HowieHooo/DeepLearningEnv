{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note1 Intro to tensorflow\n",
    "\n",
    "#### 1.Numbers or letters are not store in integers or floats or strings, instead, there are store as as \"tensors\", which is the basic data type in tensorflow(zhangliang)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "for constants, use tf.costants()\n",
    "e.x.\n",
    "A = tf.constants(1) A is a 0 demention int32 tensor\n",
    "\n",
    "\n",
    "#### 2.Session is the enviroment to run a computational graph, written as:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run()\n",
    "        \n",
    "#### 3.If the tensor is not a constant, we have to use a placeholder to hold for the tensor value, and assign it with specific value before the session runs(because session handles the operation to the GPU or CPU, this suits fine)\n",
    "ex\n",
    "A = tf.placeholder(int32)\n",
    "Note: If you want to assign a tensor to a placeholder then you have to assign a tensortype, otherwise when you feed to it before the session run, it'll throw error.\n",
    "\n",
    "How to feed the value:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ouptut or something else = sess.run(A, feed_dict={A: 55})\n",
    "    \n",
    "#### 4.tensorflow math\n",
    "tf.add(), tf.mutiply(), tf.subtract(), tf.divide()\n",
    "for mutiple math operations, for ex:\n",
    "tf.add(tf.divide(), tf.mutiply)\n",
    "\n",
    "use tf.cast to transform integer to float\n",
    "\n",
    "#### 5.Linear functions in tensorflow\n",
    "\n",
    "y = wx + b\n",
    "\n",
    "because w and b need to be changed in time, we use tf.variable() to store them, two things must know:\n",
    "1.must give it a init value, for w we can use a normal distribution(also protect from overfitting), for b we can use a tf.zero\n",
    "\n",
    "2.these variables stored their states in the session so, you have to initialize them in the session,\n",
    "use function tf.global_variables_intializer() to init their status\n",
    "\n",
    "3.intialize w: w = tf.Variable(tf.truncated_normal((number of features, number of labels)))\n",
    "  intialize b: b = tf.Variable(tf.zeros(number of labels))\n",
    "  \n",
    "#### 6.Softmax and one hot encoding\n",
    "\n",
    "1.softmax, called logits, being used to squash the input into probilities which will be added to one(change the result into probilities)\n",
    "2.then use one hot encoding to change the probilities into labels, like 1 or 0, for the convenience to output our prediction\n",
    "code:\n",
    "tf.nn.softmax()\n",
    "code:\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.Labelbinarizer()\n",
    "lb.fit(the input)\n",
    "lb.transform(the input)\n",
    "\n",
    "\n",
    "# Project 2 concerns\n",
    "#### one hot encoding, there are more questions to ask about:\n",
    "1.why is the labels shape changed?\n",
    "2. after passing a global variable and run it the second time, it still changes the shape?\n",
    "3.lb.classes_ = np.array(list(range(10))) whats the meaning of this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
